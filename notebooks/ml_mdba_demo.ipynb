{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage\n",
    "import xarray as xr\n",
    "from rasterstats import zonal_stats\n",
    "from shapely.geometry import shape\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from eo_insights.spatial import xr_vectorize\n",
    "from eo_insights.raster_base import RasterBase, QueryParams, LoadParams\n",
    "from eo_insights.stac_configuration import de_australia_stac_config\n",
    "from eo_insights.band_indices import calculate_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = Path.cwd()\n",
    "DATA_PATH = CURRENT_DIR.parent / \"data\" / \"mdba\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate subset of data\n",
    "Use river red gums and black box in Victoria only.\n",
    "\n",
    "Skip to next section if subset has already been generated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_data = gpd.read_file(DATA_PATH / \"all_states_concatenated.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_subset = veg_species_data[\n",
    "    veg_species_data[\"sciname\"].str.contains(\n",
    "        \"Eucalyptus camaldulensis|Eucalyptus largiflorens\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from scientific names to common names\n",
    "mapping = {\n",
    "    \"Eucalyptus camaldulensis\": \"river red gum\",\n",
    "    \"Eucalyptus largiflorens\": \"black box\",\n",
    "}\n",
    "\n",
    "# Iterate over the mapping and update the 'common_name' column accordingly\n",
    "for key, value in mapping.items():\n",
    "    mask = veg_species_subset[\"sciname\"].str.contains(key, case=False, na=False)\n",
    "    veg_species_subset.loc[mask, \"commonname\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_vic = veg_species_subset[veg_species_subset[\"statedb\"] == \"vic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_vic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_vic.explore(column=\"commonname\", cmap=[\"red\", \"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_vic.to_file(\n",
    "    DATA_PATH / \"vic_twospecies.gpkg\", driver=\"GPKG\", layer=\"commonname\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up training and testing data\n",
    "\n",
    "input: xarray data to use for segmentation, xarray data to use for features, training/testing polygons/points/labels\n",
    "output: X_train, y_train, X_test, y_test\n",
    "\n",
    "Steps:\n",
    "1. Get train and test datasets\n",
    "2. Load segmentation data\n",
    "3. Run segmentation to get vectors\n",
    "4. Match segmentation vectors with occurence points (must be done for train and test, separate because in my case, the data are in two locations)\n",
    "5. Clean up training data (e.g. randomly sample from \"other\")\n",
    "6. Load training data from STAC\n",
    "7. Run zonal statistics\n",
    "8. Prepare data for sklearn classifier\n",
    "9. Train on training data\n",
    "10. Predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_species_vic = gpd.read_file(DATA_PATH / \"vic_twospecies.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gpd.read_file(DATA_PATH / \"vic_model_areas.gpkg\")\n",
    "\n",
    "areas.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = gpd.sjoin(\n",
    "    veg_species_vic,\n",
    "    areas[areas[\"purpose\"] == \"train\"].to_crs(veg_species_vic.crs),\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "testing_data = gpd.sjoin(\n",
    "    veg_species_vic,\n",
    "    areas[areas[\"purpose\"] == \"test\"].to_crs(veg_species_vic.crs),\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load data for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmentation_data(bounding_box):\n",
    "    # Must return an xarray DataArray\n",
    "\n",
    "    # Load Sentinel-2 for 2019 - 2020\n",
    "    segmentation_raster = RasterBase.from_stac_query(\n",
    "        config=de_australia_stac_config,\n",
    "        collections=[\"ga_s2am_ard_3\", \"ga_s2bm_ard_3\"],\n",
    "        query_params=QueryParams(\n",
    "            bbox=bounding_box,\n",
    "            start_date=\"2019-12-01\",\n",
    "            end_date=\"2020-11-30\",\n",
    "        ),\n",
    "        load_params=LoadParams(\n",
    "            crs=\"EPSG:3577\", resolution=30, bands=[\"red\", \"nir\", \"fmask\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Calculate NDVI\n",
    "    segmentation_raster.data = calculate_indices(segmentation_raster.data, [\"ndvi\"])\n",
    "\n",
    "    # Apply masking in-place - disabled for now, as there were still nan values after doing medians.\n",
    "    # segmentation_raster.apply_mask(\"fmask\", nodata=np.nan)\n",
    "\n",
    "    # Select only NDVI and resample\n",
    "    segmentation_data = (\n",
    "        segmentation_raster.data[\"ndvi\"].resample(time=\"1QS-Dec\").median().compute()\n",
    "    )\n",
    "\n",
    "    return segmentation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_segmentation_data = load_segmentation_data(training_data.total_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation(segmentation_data):\n",
    "    # Identify dimensions:\n",
    "    # dims_dict = {dim: size for dim, size in zip(segmentation_data.dims, segmentation_data.shape)}\n",
    "    output_dims = (\"y\", \"x\")\n",
    "    output_coords = {\n",
    "        \"y\": segmentation_data.coords[\"y\"],\n",
    "        \"x\": segmentation_data.coords[\"x\"],\n",
    "    }\n",
    "    output_attrs = segmentation_data.attrs\n",
    "\n",
    "    segmentation_data_np = segmentation_data.transpose(\"y\", \"x\", \"time\").data\n",
    "\n",
    "    segments = skimage.segmentation.quickshift(\n",
    "        segmentation_data_np,\n",
    "        ratio=1.0,\n",
    "        kernel_size=2,\n",
    "        max_dist=10,\n",
    "        sigma=0,\n",
    "        convert2lab=False,\n",
    "        rng=42,\n",
    "    )\n",
    "\n",
    "    xr_segments = xr.DataArray(\n",
    "        segments, coords=output_coords, dims=output_dims, attrs=output_attrs\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    gdf_segments = xr_vectorize(xr_segments)\n",
    "\n",
    "    return gdf_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_segments = run_segmentation(training_segmentation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Match segmentation vectors with occurrence points\n",
    "\n",
    "Assign the species label as the most common species that appears in the segment.\n",
    "If no species labels are present, assign the label \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_segments_to_points(segments, points, label_column):\n",
    "\n",
    "    segments_with_points = gpd.sjoin(\n",
    "        segments,\n",
    "        points[[label_column, \"geometry\"]].to_crs(segments.crs),\n",
    "        how=\"left\",\n",
    "        op=\"contains\",\n",
    "    )\n",
    "\n",
    "    # For all segments with no points available, replace with \"other\"\n",
    "    segments_with_points[label_column] = segments_with_points[label_column].replace(\n",
    "        np.nan, \"other\"\n",
    "    )\n",
    "\n",
    "    modal_label = (\n",
    "        segments_with_points[[\"attribute\", \"geometry\", label_column]]\n",
    "        .groupby(\"attribute\")[label_column]\n",
    "        .apply(lambda x: x.mode())\n",
    "        .reset_index(0)\n",
    "    )\n",
    "\n",
    "    segments_labelled = segments.merge(modal_label)\n",
    "\n",
    "    return segments_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_polygons = match_segments_to_points(\n",
    "    training_segments, training_data, \"commonname\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_polygons.explore(column=\"commonname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Clean up the training data\n",
    "\n",
    "Keep all samples for the two species classes.\n",
    "Limit the number of samples in the \"other\" class to 30 to help with balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_red_gum = training_polygons[training_polygons[\"commonname\"] == \"river red gum\"]\n",
    "black_box = training_polygons[training_polygons[\"commonname\"] == \"black box\"]\n",
    "other = training_polygons[training_polygons[\"commonname\"] == \"other\"].sample(n=30)\n",
    "\n",
    "balanced_training_polygons = pd.concat([river_red_gum, black_box, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_training_polygons.explore(column=\"commonname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Load data for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_data(bounding_box):\n",
    "    # Must return an xarray Dataset\n",
    "\n",
    "    # Load Sentinel-2 for 2019 - 2020\n",
    "    eo_feature_raster = RasterBase.from_stac_query(\n",
    "        config=de_australia_stac_config,\n",
    "        collections=[\"ga_s2am_ard_3\", \"ga_s2bm_ard_3\"],\n",
    "        query_params=QueryParams(\n",
    "            bbox=bounding_box,\n",
    "            start_date=\"2019-12-01\",\n",
    "            end_date=\"2020-11-30\",\n",
    "        ),\n",
    "        load_params=LoadParams(\n",
    "            crs=\"EPSG:3577\",\n",
    "            resolution=30,\n",
    "            bands=[\"red\", \"green\", \"blue\", \"nir\", \"fmask\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Calculate NDVI\n",
    "    eo_feature_raster.data = calculate_indices(eo_feature_raster.data, [\"ndvi\"])\n",
    "\n",
    "    # Do medians\n",
    "    eo_feature_raster.data = (\n",
    "        eo_feature_raster.data.resample(time=\"1QS-Dec\").median().compute()\n",
    "    )\n",
    "\n",
    "    # Load the DEM - disabled for now as the two arrays can't be easily concatenated right now.\n",
    "    # Might need to output as a list and then run zonal stats on all\n",
    "    dem_feature_raster = RasterBase.from_stac_query(\n",
    "        config=de_australia_stac_config,\n",
    "        collections=[\"ga_srtm_dem1sv1_0\"],\n",
    "        query_params=QueryParams(\n",
    "            bbox=bounding_box,\n",
    "            start_date=\"2014\",\n",
    "            end_date=\"2014\",\n",
    "        ),\n",
    "        load_params=LoadParams(\n",
    "            crs=\"EPSG:3577\",\n",
    "            resolution=30,\n",
    "            bands=[\"dem_s\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dem_feature_raster.data = dem_feature_raster.data.compute()\n",
    "\n",
    "    # feature_raster_data = xr.concat[[eo_feature_raster.data, dem_feature_raster.data]]\n",
    "\n",
    "    # Apply masking in-place - disabled for now, as there were still nan values after doing medians.\n",
    "    # eo_feature_raster.apply_mask(\"fmask\", nodata=np.nan)\n",
    "\n",
    "    return [eo_feature_raster.data, dem_feature_raster.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = load_feature_data(training_data.total_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zonal_stats(\n",
    "    feature_datasets: list[xr.Dataset],\n",
    "    geometries,\n",
    "    zonalstats_list: list[str] = [\"median\", \"std\", \"percentile_10\", \"percentile_90\"],\n",
    "):\n",
    "\n",
    "    final_gdf = None\n",
    "\n",
    "    for feature_data in feature_datasets:\n",
    "        n_timesteps = feature_data.dims.get(\"time\")\n",
    "\n",
    "        for timestep in range(n_timesteps):\n",
    "            print(f\"Computing stats for timestep = {timestep}\")\n",
    "\n",
    "            timestep_xr = feature_data.isel(time=timestep).squeeze()\n",
    "\n",
    "            for band_name in list(timestep_xr.keys()):\n",
    "\n",
    "                print(f\"    Computing stats for {band_name}\")\n",
    "\n",
    "                data = timestep_xr[band_name].data\n",
    "                zonalstats = zonal_stats(\n",
    "                    geometries,\n",
    "                    data,\n",
    "                    stats=zonalstats_list,\n",
    "                    all_touched=True,\n",
    "                    geojson_out=True,\n",
    "                    affine=timestep_xr.odc.affine,\n",
    "                )\n",
    "\n",
    "                df_stats = pd.DataFrame.from_dict(zonalstats)\n",
    "                geoms = [shape(j) for j in df_stats[\"geometry\"]]\n",
    "                gdf_stats = gpd.GeoDataFrame(\n",
    "                    df_stats, geometry=geoms, crs=timestep_xr.odc.crs\n",
    "                )\n",
    "\n",
    "                for stat in zonalstats_list:\n",
    "                    if n_timesteps > 1:\n",
    "                        stat_var = f\"{band_name}_{timestep}_{stat}\"\n",
    "                    else:\n",
    "                        stat_var = f\"{band_name}_{stat}\"\n",
    "                    gdf_stats[stat_var] = [\n",
    "                        gdf_stats[\"properties\"][j][stat] for j in range(len(gdf_stats))\n",
    "                    ]\n",
    "\n",
    "                gdf_stats = gdf_stats.drop([\"properties\", \"type\", \"bbox\"], axis=1)\n",
    "\n",
    "                if final_gdf is None:\n",
    "                    final_gdf = gdf_stats.copy()\n",
    "                else:\n",
    "                    final_gdf = pd.concat(\n",
    "                        [final_gdf, gdf_stats.drop([\"id\", \"geometry\"], axis=1)], axis=1\n",
    "                    )\n",
    "\n",
    "    return final_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = get_zonal_stats(\n",
    "    feature_data,\n",
    "    training_polygons.geometry,\n",
    "    zonalstats_list=[\"median\", \"std\", \"percentile_10\", \"percentile_90\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the training polygons to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_segments = training_features.merge(\n",
    "    training_polygons[[\"geometry\", \"commonname\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Prepare data for sklearn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_segments.drop(columns=[\"id\", \"geometry\"])\n",
    "\n",
    "y_train = training_data[\"commonname\"]\n",
    "X_train = training_data.drop([\"commonname\"], axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "print(le.classes_)\n",
    "y_train_transformed = le.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Train the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier = RandomForestClassifier(n_estimators=200)\n",
    "Classifier.fit(X_train, y_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = training_data.drop([\"commonname\"], axis=1).columns\n",
    "feat_importance_indices = np.argsort(Classifier.feature_importances_)\n",
    "feat_importance_ordered = np.array(column_names)[feat_importance_indices]\n",
    "print(\"Top 10 features: \\n\", feat_importance_ordered[0:10])\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(\n",
    "    y=feat_importance_ordered[0:10],\n",
    "    width=Classifier.feature_importances_[feat_importance_indices][0:10],\n",
    ")\n",
    "plt.gca().set_ylabel(\"Importance\", labelpad=6)\n",
    "plt.gca().set_xlabel(\"Variable\", labelpad=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Repeat above steps to prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading segmentation data\")\n",
    "testing_segmentation_data = load_segmentation_data(testing_data.total_bounds)\n",
    "\n",
    "print(\"Running segmentation\")\n",
    "testing_segments = run_segmentation(testing_segmentation_data)\n",
    "\n",
    "print(\"Matching segments to points\")\n",
    "testing_polygons = match_segments_to_points(\n",
    "    testing_segments, testing_data, \"commonname\"\n",
    ")\n",
    "\n",
    "print(\"Loading feature data\")\n",
    "test_feature_data = load_feature_data(testing_data.total_bounds)\n",
    "\n",
    "print(\"Getting zonal statistics\")\n",
    "testing_features = get_zonal_stats(\n",
    "    test_feature_data,\n",
    "    testing_polygons.geometry,\n",
    "    zonalstats_list=[\"median\", \"std\", \"percentile_10\", \"percentile_90\"],\n",
    ")\n",
    "\n",
    "print(\"Assembling final dataset for ML\")\n",
    "testing_input = testing_features.merge(testing_polygons[[\"geometry\", \"commonname\"]])\n",
    "\n",
    "print(\"Saving test data\")\n",
    "testing_input.to_file(DATA_PATH / \"test_samples.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test data for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_prediction_input = testing_input.drop(columns=[\"id\", \"geometry\"])\n",
    "\n",
    "y_test = testing_prediction_input[\"commonname\"]\n",
    "X_test = testing_prediction_input.drop([\"commonname\"], axis=1)\n",
    "\n",
    "y_test_transformed = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predictions and append to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = Classifier.predict(X_test)\n",
    "\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "predicted_segments = testing_input.join(\n",
    "    pd.Series(y_test_pred_labels, name=\"commonname_prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy metrics and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "f1_metric = f1_score(y_test_transformed, y_test_pred, average=\"macro\")\n",
    "accuracy_metric = accuracy_score(y_test_transformed, y_test_pred)\n",
    "\n",
    "print(f\"F1-Score = {f1_metric}\")\n",
    "print(f\"Accuracy Score = {accuracy_metric}\")\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_transformed, y_test_pred)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predictions on map, along with species sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = predicted_segments[[\"commonname\", \"commonname_prediction\", \"geometry\"]].explore(\n",
    "    column=\"commonname_prediction\"\n",
    ")\n",
    "testing_data.explore(column=\"commonname\", cmap=[\"red\", \"green\"], m=m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoinsights",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
